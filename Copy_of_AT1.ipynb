{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AT1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katyalrajat/UTS_ML2019_ID13132903/blob/master/Copy_of_AT1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o6pp0RXaMQC",
        "colab_type": "text"
      },
      "source": [
        "##Draft and Experiment Area\n",
        "\n",
        "### Selecting a paper (Motivation)\n",
        "\n",
        "There were many interesting papers available in the paper list. After glancing through and reading the abstracts, a few paper interested me, these were:\n",
        "\n",
        "1. A Mathematical Theory of Communication \n",
        "  By C. E. SHANNON\n",
        "\n",
        "2. Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection \n",
        "  by Peter N. Belhumeur, Jo~ao P. Hespanha, and David J. Kriegman\n",
        "\n",
        "3. Generative Adversarial Nets\n",
        "  by Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\n",
        "  Sherjil Ozairy, Aaron Courville, Yoshua Bengioz\n",
        "\n",
        "4. Gradient Based Learning Applied to Document Recognition\n",
        "  by Yann LeCun,  Leon Bottou, Yoshua Bengio and Patrick Haner\n",
        "  \n",
        "  \n",
        "  The most elegant paper with great detail seemed to be the first one by Shannon, it spoke about the process of communication via a network, on a fundamental level in binary sequence. It stated the challenges of different communcation scenarios and how to overcome them. But for this assigment, I wanted to review a paper which is more in lines with the practical applications of data sciecnce that I may encounter during/after the masters. So I choose a few papers [2-4] that were related to Deep Learning and Image recognition, as i was interested in knowing more about these topics.\n",
        "  \n",
        " While the Eigenfaces vs Fisherfaces seemed a good paper, it seemed to be talking about facial recognition techniques only and I was looking for a more geenralised concept. Finally paper 3 and 4 seemed to be the on the lines of the current state deep learning solutions. After doing a quick scan and browsing these concepts it seemed that GAN is less generic concept than Neural Networks and going through some image recognition videos like the one below from Stanford, I realised that CNN in 2012 was a break-through moment in Image recogonition and so I dedcided to read more about that.\n",
        " \n",
        " Here I would review: Gradient Based Learning Applied to Document Recognition as it seems to be the foundation for the 2012 ImageNet Classification with Deep Convolutional Neural Networks research by Alex Krizhevsky et. al\n",
        " \n",
        "\n",
        " Other resources :\n",
        "\n",
        " https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=2&t=0s\n",
        "\n",
        " https://qiita.com/Rowing0914/items/445095163bc6a58c59b5#3-gradient-backpropagation\n",
        " \n",
        " https://slideplayer.com/slide/13169533/  \n",
        " \n",
        " http://cs231n.stanford.edu/\n",
        " \n",
        " https://dataconomy.com/2017/04/history-neural-networks/\n",
        " \n",
        " https://www.youtube.com/watch?v=m3BrTjo2zUA\n",
        " \n",
        " https://www.youtube.com/watch?v=ZjM_XQa5s6s\n",
        " \n",
        " https://deeplizard.com/learn/video/YRhxdVk_sIs\n",
        " \n",
        " \n",
        "### Paper Sections \n",
        " \n",
        "Sec I: Intro .\n",
        "\n",
        "Sec II: CNN .\n",
        "\n",
        "Sec III: Results and Comparison with Other Methods. \n",
        "\n",
        "Sec IV: Multi Module Systems and Graph Transformer Networks.\n",
        "\n",
        "Sec. V : Multiple Object Recognition- Heuristic Over Segmentation.\n",
        "\n",
        "Sec. VI : Global Training for Graph Transformer Networks.\n",
        "\n",
        "Sec. VII : Space displacement NN fro multiple object recognotion.\n",
        "\n",
        "Sec. VIII : Graph Transformer Networks and Transducers.\n",
        "\n",
        "Sec IX : An On Line Handwriting Recognition System .\n",
        "\n",
        "Sec X : A Check Reading System.\n",
        "\n",
        "Sec XI : Conclusions\n",
        "\n",
        " \n",
        "\n",
        "### Introduction Notes (roughwork)\n",
        "\n",
        "The paper aims to show the techniques by which a machine learning algorithm for pattern recogonition, using hand-written notes as an example, could outperform the traditional hand-crafted techniques.\n",
        "\n",
        "Traditional Approach :\n",
        "\n",
        "Module a. Feature Extractor: Hand-crafted and customised according to the applicaiton\n",
        "Module b. Classifier: General purpose and trainanble\n",
        "\n",
        "New approach: Use GTN instead of creating customised feature extractor for every use case\n",
        "\n",
        "\n",
        "A. Data based Classifier:\n",
        "\n",
        "***Error(Etest - Etrain) increases with more training samples and decreases with higher complexity(h)***\n",
        "Algorithms use structural risk minimization\n",
        "\n",
        "B. Gradient Based Learning: E(W) is a smooth function of W.\n",
        "\n",
        "C. Gradient Back Propagation : Gradient learning is old concept (1950s) but is now applied to machine learning algorithms like neural networks using back propagation (output to input).\n",
        "\n",
        "D. Learning in Real Handwriting Recognition Systems: \n",
        "\n",
        "*   Heuristics is the technique of separating charaters using edges.\n",
        "*   Alternately training at a whole string level utilizing a loss function\n",
        "*   Alternately use character spottinh and feed it to a GTN\n",
        "\n",
        "E Globally Trainable Systems:\n",
        "\n",
        "Traditionally system contains:\n",
        "\n",
        "*   Field Locator\n",
        "*   Field Segmenter\n",
        "*   recognizer\n",
        "*   contextual post-processor\n",
        "\n",
        "where each phase is intergrated and a subset of parameters is optimised at the last step.\n",
        "\n",
        "Globally trained system minimises the global error and uses gradient based learning approach to find a minimum with respect to all parameters. This is done by having all the functions as differentiable and continous to be able to find the minima for each parameter.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### CNN:\n",
        "\n",
        "3 Invariance properties: \n",
        "\n",
        "*   Shift\n",
        "*   Scale\n",
        "*   Distortion\n",
        "\n",
        "Architecture:\n",
        "\n",
        " Layers >> Planes (Feature maps) >> Units of same weight\n",
        " \n",
        " The Set of units' output is a Feature map.\n",
        " A single unit's dimensions is it's receptive field.\n",
        " \n",
        "An interesting property of convolutional layers is that\n",
        "if the input image is shifted the feature map output will\n",
        "be shifted by the same amount.\n",
        "\n",
        " \n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyAD1ZxhbVbM",
        "colab_type": "text"
      },
      "source": [
        "## Review Report on \"Gradient Based Learning Applied to Document Recognition\" by Yann LeCun, Leon Bottou, Yoshua Bengio and Patrick Haner , 1998\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSd9lZLIa9HS",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "xxx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeBEO3Tk1Aah",
        "colab_type": "text"
      },
      "source": [
        "### Content\n",
        "\n",
        "The central problem that the paper aims to solve is to have an accurate and fast method of generalised pattern recognition. Such a method should be replicable for tasks like voice recognition, image/object recognition and document text recognition. This paper shows the advantages of relying on automated learning instead of hand-crafted feature extractors for recognition tasks. The paper also shows the advantages of using a globally trained approach to a machine learning algorithms rather than optimising individual modules and integrating them. \n",
        "\n",
        "In this paper, the authors take the problem of document recogonition and compare the different approaches used and their accuracy results. The authors highlight the advantages of using a convolutional neural network(CNN). The CNN is trained in a gradient based approach made possible using back-propagation techniques. The authors highlight the properties of using a CNN such as inavariance in scale, shift and distortion while explaining the architecture of a 7 layered CNN (LeNet-5).\n",
        "\n",
        "The results from the testing of a sample of 10,000 images of 32x32 pixels, show that the Boosted LeNetâˆ’4, a CNN, outperforms all other algorithms in accuracy scores while other CNNs like LeNet-5 and Support Vector Machines(SVM) perform almost as well. But the CNNs have a much lower memory usage (less than 10 times) and outperform other algorithms like SVM in computational time. The authors argue that the advantages of using this CNN architecture becomes much more evident with increase in training data sizes.\n",
        "\n",
        "The other sections of the paper explain the concept of Graph Transfer Network(GTN) using multiple modules and a back-propagation technique similar to one used in LeNet-5. The paper discusses in detail the techniques of Multiple Object recognition using heuristics over segmentation. It also discussed the Global Training scenarions in GTN in depth. \n",
        "\n",
        "Finally, the authors discuss how these concpets are applied to a cheque reading system that is live and deployed in American banks which reads over a million cheques per day.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-h1ZILH1KL_",
        "colab_type": "text"
      },
      "source": [
        "### Innovation\n",
        "\n",
        "\n",
        "Although a few algorithms before this period used neural networks for pattern recognition using self-learning methods, namely the [Cognitron](https://link.springer.com/article/10.1007/BF00342633) , [Neocognitron](https://link.springer.com/article/10.1007/BF00344251) and [Morsel](https://psycnet.apa.org/record/1991-98206-000) , the ***LeNet-5 Architecture*** (shown in the image below) was unqiue.\n",
        "\n",
        "VS Cognitron [copied]\n",
        "\n",
        "The cognitron discussed in this paper is not\n",
        "intended to be a complete system for pattern recognition: If we want to make a pattern recognizer with a\n",
        "cognitron, some other functions must be added to it.\n",
        "For instance, a feature extractor with a function of\n",
        "normalization of position, size, etc. would be necessary\n",
        "to be added in front of the cognitron, and a decision\n",
        "circuit should be cascaded after the last layer of the\n",
        "cognitron. \n",
        "\n",
        "\n",
        "*** Loss Funciton ***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![LeNet-5](https://cdn-images-1.medium.com/max/800/0*V1vb9SDnsU1eZQUy.jpg)\n",
        "\n",
        "\n",
        "The LeNet-5 , by LeCunn has 8 layers in total(including the input layer). \n",
        "It uses Convolutional technique for feature learning which is carried out by a receptive field. The receptive field in the first step is a 5x5 matrix which scans the entire input image while performing an operation like a dot product and stores it's output as a feature. Different functions can be generated by changing the values of the receptive field matrix as the same operation with the input image will generate different results. The outputs are passed to an additive bias and a squashing function to generate the feature map. Here 6 such receptive fields are used in the first step to generate a 6 feature maps of 28x28 size. \n",
        "\n",
        "In order to better visualise this step of convolutionaling of the recepter field, you can see the gif below.\n",
        "Here the blue section is the input image, white boudary is the padding. The receptor field unit is a 3x3 shaded matrix that moves and adds the operation output value to the green field which is the next layer carrying the feature map.\n",
        "\n",
        "![](https://deeplizard.com/images/same_padding_no_strides.gif)\n",
        "\n",
        "The layer after that utilises a subsampling tecnique which is used to highlight the results of the feature learning. This is done by using a 2x2 matrix and a pooling operation. Thus effectively reducing image resolution by half, creating 14x14 size feature maps which better highlight features like edges. Similar to our first step, the next layers creates 16 feature maps using the previous layer as input and 5x5 receptive field for convulation. \n",
        "Similary next layers are subsampling and convolutional layer. The final output layer consists of a Radical Bias Function with 84 inputs from the previous layer.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rarIvWIa1MGf",
        "colab_type": "text"
      },
      "source": [
        "### Technical Quality\n",
        "\n",
        "Appropriate usage of the standard practices , example testing scenarios with diff algos -> accuraccy, processing memoy. \n",
        "\n",
        "Equestions mentioned like squashing function, radical bias funciton\n",
        "\n",
        "Technical concepts like Loss fucntion explained in detail.\n",
        "\n",
        "Replicabale in nature as referenced by ImageNet algo in 2012 and by others beyond."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn1LcrYi1PUf",
        "colab_type": "text"
      },
      "source": [
        "### Application and X-factor\n",
        "\n",
        "Talk about the 2012 imagenet compition and research based on this paper.\n",
        "Give the comparison and how many times it's been cited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "law981ws1Unn",
        "colab_type": "text"
      },
      "source": [
        "### Presentation\n",
        "\n",
        "\n",
        "I really liked how the paper has effecively referenced every concept that has been introuced in order to dig deeper into it's foundations. This allows the reader to dig much deeper into something that they don't undertsand or something that they feel particularly interested in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyg2UC3R1ZXd",
        "colab_type": "text"
      },
      "source": [
        "### References"
      ]
    }
  ]
}