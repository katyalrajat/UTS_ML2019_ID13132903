{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of AT1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katyalrajat/UTS_ML2019_ID13132903/blob/master/Copy_of_Copy_of_AT1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o6pp0RXaMQC",
        "colab_type": "text"
      },
      "source": [
        "##Draft and Experiment Area\n",
        "\n",
        "**Selecting a paper (Motivation):**\n",
        "\n",
        "There were many interesting papers available in the paper list. After glancing through and reading the abstracts, a few paper interested me, these were:\n",
        "\n",
        "1. A Mathematical Theory of Communication \n",
        "  By C. E. SHANNON\n",
        "\n",
        "2. Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection \n",
        "  by Peter N. Belhumeur, Jo~ao P. Hespanha, and David J. Kriegman\n",
        "\n",
        "3. Generative Adversarial Nets\n",
        "  by Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\n",
        "  Sherjil Ozairy, Aaron Courville, Yoshua Bengioz\n",
        "\n",
        "4. Gradient Based Learning Applied to Document Recognition\n",
        "  by Yann LeCun,  Leon Bottou, Yoshua Bengio and Patrick Haner\n",
        "  \n",
        "  \n",
        "  The most elegant paper with great detail seemed to be the first one by Shannon, it spoke about the process of communication via a network, on a fundamental level in binary sequence. It stated the challenges of different communication scenarios and how to overcome them. But for this assignment, I wanted to review a paper which is more in lines with the practical applications of data science that I may encounter during/after the masters. So, I choose a few papers [2-4] that were related to Deep Learning and Image recognition, as I was interested in knowing more about these topics.\n",
        "  \n",
        " While the Eigenfaces vs Fisherfaces seemed a good paper, it seemed to be talking about facial recognition techniques only and I was looking for a more generalised concept. Finally paper 3 and 4 seemed to be the on the lines of the current state deep learning solutions. After doing a quick scan and browsing these concepts it seemed that GAN is a less generic concept than Neural Networks and going through some image recognition videos like the one below from Stanford, I realised that CNN in 2012 was a break-through moment in Image recognition and so I decided to read more about that.\n",
        " \n",
        " Here I would review: Gradient Based Learning Applied to Document Recognition as it seems to be the foundation for the 2012 ImageNet Classification with Deep Convolutional Neural Networks research by Alex Krizhevsky et. al\n",
        "\n",
        "\n",
        " \n",
        " **Other resources :**\n",
        "\n",
        " https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=2&t=0s\n",
        "\n",
        " https://qiita.com/Rowing0914/items/445095163bc6a58c59b5#3-gradient-backpropagation\n",
        " \n",
        " https://slideplayer.com/slide/13169533/  \n",
        " \n",
        " http://cs231n.stanford.edu/\n",
        " \n",
        " https://dataconomy.com/2017/04/history-neural-networks/\n",
        " \n",
        " https://www.youtube.com/watch?v=m3BrTjo2zUA\n",
        " \n",
        " https://www.youtube.com/watch?v=ZjM_XQa5s6s\n",
        " \n",
        " https://deeplizard.com/learn/video/YRhxdVk_sIs\n",
        " \n",
        " \n",
        "**Paper Sections:**\n",
        "\n",
        "\n",
        "\n",
        "1. Intro\n",
        "2.CNN\n",
        "3.Results and Comparison with Other Methods\n",
        "4.Multi Module Systems and Graph Transformer Networks\n",
        "5.Multiple Object Recognition- Heuristic Over Segmentation\n",
        "6.Global Training for Graph Transformer Networks\n",
        "7.Space displacement NN for multiple object recognition\n",
        "8.Graph Transformer Networks and Transducers\n",
        "9.An Online Handwriting Recognition System\n",
        "10.A Check Reading System\n",
        "11.Conclusions\n",
        "\n",
        "\n",
        "**Introduction Notes (roughwork):**\n",
        "\n",
        "The paper aims to show the techniques by which a machine learning algorithm for pattern recognition, using hand-written notes as an example, could outperform the traditional hand-crafted techniques.\n",
        "\n",
        "Traditional Approach :\n",
        "Module a. Feature Extractor: Hand-crafted and customised according to the application\n",
        "Module b. Classifier: General purpose and trainable\n",
        "\n",
        "New approach: Use GTN instead of creating customised feature extractor for every use case\n",
        "A. Data based Classifier:\n",
        "***Error(Etest - Etrain) increases with more training samples and decreases with higher complexity(h)***\n",
        "Algorithms use structural risk minimization\n",
        "\n",
        "B. Gradient Based Learning: E(W) is a smooth function of W.\n",
        "\n",
        "C. Gradient Back Propagation : Gradient learning is old concept (1950s) but is now applied to machine learning algorithms like neural networks using back propagation (output to input).\n",
        "\n",
        "D. Learning in Real Handwriting Recognition Systems: \n",
        "\n",
        "*   Heuristics is the technique of separating characters using edges.\n",
        "*   Alternately training at a whole string level utilizing a loss function\n",
        "*   Alternately use character spotting and feed it to a GTN\n",
        "\n",
        "E Globally Trainable Systems:\n",
        "\n",
        "Traditionally system contains:\n",
        "\n",
        "*   Field Locator\n",
        "*   Field Segmenter\n",
        "*   recognizer\n",
        "*   contextual post-processor\n",
        "\n",
        "where each phase is integrated, and a subset of parameters is optimised at the last step.\n",
        "\n",
        "Globally trained system minimises the global error and uses gradient based learning approach to find a minimum with respect to all parameters. This is done by having all the functions as differentiable and continuous to be able to find the minima for each parameter.\n",
        "\n",
        "**CNN**\n",
        "\n",
        "3 Invariance properties: \n",
        "\n",
        "*   Shift\n",
        "*   Scale\n",
        "*   Distortion\n",
        "\n",
        "Architecture:\n",
        "\n",
        " Layers >> Planes (Feature maps) >> Units of same weight\n",
        " \n",
        " The Set of units' outputs is a Feature map.\n",
        " A single unit's dimensions are it's receptive field.\n",
        " \n",
        "An interesting property of convolutional layers is that\n",
        "if the input image is shifted the feature map output will\n",
        "be shifted by the same amount.\n",
        "\n",
        "xxx---xxx---xxx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyAD1ZxhbVbM",
        "colab_type": "text"
      },
      "source": [
        "## Review Report on \"Gradient Based Learning Applied to Document Recognition\" by Yann LeCun, Leon Bottou, Yoshua Bengio and Patrick Haner , 1998\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSd9lZLIa9HS",
        "colab_type": "text"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "This document aims to review the titled research paper. The main areas of review would be its Content, Innovation, Technical quality, Application, X-factor and finally the presentation. I would like to mention that I am not an expert in reviewing research work and the comments on these sections are merely personal observations. The motivation behind selecting this paper could be found in the Draft Area above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeBEO3Tk1Aah",
        "colab_type": "text"
      },
      "source": [
        "### Content\n",
        "\n",
        "The central problem that the paper aims to solve is to have an accurate and fast method of generalised pattern recognition. Such a method should be replicable for tasks like voice recognition, image/object recognition and document text recognition. This paper shows the advantages of relying on automated learning instead of hand-crafted feature extractors for recognition tasks. The paper also shows the advantages of using a globally trained approach to a machine learning algorithm rather than optimising individual modules and integrating them. \n",
        "\n",
        "In this paper, the authors take the problem of document recognition and compare the different approaches used and their accuracy results. The authors highlight the advantages of using a convolutional neural network(CNN). The CNN is trained in a gradient based approach made possible using back-propagation techniques. The authors highlight the properties of using a CNN such as invariance in scale, shift and distortion while explaining the architecture of a 7 layered CNN (LeNet-5).\n",
        "\n",
        "The results from the testing of a sample of 10,000 images of 32x32 pixels, show that the Boosted LeNetâˆ’4, a CNN, outperforms all other algorithms in accuracy scores for single character recognition while other CNNs like LeNet-5 and Support Vector Machines(SVM) perform almost as well. But the CNNs have a much lower memory usage (less than 10 times) and outperform other algorithms like SVM in computational cost. The authors argue that the advantages of using this CNN architecture becomes much more evident with increase in training data sizes.\n",
        "\n",
        "The other sections of the paper explain the concept of Graph Transfer Network(GTN) using multiple modules and a back-propagation technique like one used in LeNet-5. The paper discusses in detail the techniques of Multiple Object recognition using heuristics over segmentation with the loss function architecture. It also discussed the Global Training scenarios in GTN in depth. These techniques were used to enhance the CNN for character string recognition rather than single character recognition and effectively create a Space Displacement Neural Network(SDNN).\n",
        "\n",
        "Finally, the authors discuss how these concepts are applied to a cheque reading system that is live and deployed in American banks which reads over a million cheques per day.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-h1ZILH1KL_",
        "colab_type": "text"
      },
      "source": [
        "### Innovation\n",
        "\n",
        "\n",
        "Although a few algorithms before this period used neural networks for pattern recognition using self-learning methods, namely the [Cognitron](https://link.springer.com/article/10.1007/BF00342633)[1], [Neocognitron](https://link.springer.com/article/10.1007/BF00344251)[2] and [Morsel](https://psycnet.apa.org/record/1991-98206-000)[3] , the ***LeNet-5*** (shown in the image below) was unique.\n",
        "\n",
        "For example, the Cognitron had a neural network architecture for pattern recognition but it lacked the ability to extract features with varied input sizes and positions. Also, the LeNet-5 is a complete system for application usage while the cognitron is not, as it requires a decision circuit to be cascaded at the end of its last layer [1].\n",
        "\n",
        "The closest method to solve the underlying problem at the time seemed to be the Neocognitron built by Fukushima in 1980. The paper explained how it could overcome the challenge of size and position invariance while using a self-learning neural network to recognise characters[2]. But at the time the challenge of Computer hardware memory restricted the actual simulation of such a network to only recognise 4 characters: \"X\", \"Y\", \"T\" and \"Z\".[2]\n",
        "\n",
        "The innovations in this paper that were evident were:\n",
        "\n",
        "1.   The LeNet-5 convolutional neural network Architecture\n",
        "2.   Stochastic gradient applied to weight parameters in a neural network to improve learning.\n",
        "4.   Training a Graph Transfer Network(GTN) for a string recognizer based on Heuristic \n",
        "3.   The testing of various algorithms to showcase the advantages of a convolutional neural network vs others in terms of accuracy and memory usage.\n",
        "4.   Deployment of the system to read actual cheques in real-time.\n",
        "\n",
        "The motivation behind the paper was to create real-time system that could read cheques. It seems that the paper was ***highly innovative in combining existing methods/techniques to create a new architecture/algorithm*** to solve the pattern recognition problem that can be applied to real life scenarios.\n",
        "\n",
        "While there could have been a more complex system that took inputs from each individual pixel instead of a receptive field, for two reasons that would not have been a good innovation:\n",
        "1. Lesser parameters meant less memory usage to save computational cost as at the time there were no GPUs like we have today, even CPUs were quite slow.\n",
        "2. The problem at hand was to build a cheque reader with text inputs and not an image classifier.\n",
        "\n",
        "The ***Innovative LeNet-5 Architecture***(see below) is explained in the below section.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/0*V1vb9SDnsU1eZQUy.jpg)[Figure1: LeNet-5 Architecture]\n",
        "\n",
        "\n",
        "The LeNet-5 , by LeCunn has 8 layers in total(including the input layer). \n",
        "It uses Convolutional technique for feature learning which is carried out by a receptive field. The receptive field in the first step is a 5x5 matrix which scans the entire input image while performing an operation like a dot product and stores its output as a feature. Different functions can be generated by changing the values of the receptive field matrix as the same operation with the input image will generate different results. The outputs are passed to an additive bias and a squashing function to generate the feature map. Here 6 such receptive fields are used in the first step to generate a 6 feature maps of 28x28 size. \n",
        "\n",
        "In order to better visualise this step of convolutional receptor field, you can see the gif below.\n",
        "Here the blue section is the input image, white boundary is the padding. The receptor field unit is a 3x3 shaded matrix that moves and adds the operation output value to the green field which is the next layer carrying the feature map.\n",
        "\n",
        "![](https://deeplizard.com/images/same_padding_no_strides.gif)[Figure2: 2 Layer interaction , source: deeplizard.com]\n",
        "\n",
        "The layer after that utilises a subsampling technique which is used to highlight the results of the feature learning. This is done by using a 2x2 matrix and a pooling operation. Thus, effectively reducing image resolution by half, creating 14x14 size feature maps which better highlight features like edges. Like our first step, the next layers create 16 feature maps using the previous layer as input and 5x5 receptive field for convolution. \n",
        "Similar next layers are subsampling and convolutional layer. The penultimate layer takes all the input from the C5 layer that has 120 feature maps of 1x1 size and reduces it to 84 of 1x1 size in lines with back propagation. This layer applies a loss function on the cells and passes it to the final layer which consists of a Radical Bias Function with 84 inputs from the previous layer and predicts the output class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rarIvWIa1MGf",
        "colab_type": "text"
      },
      "source": [
        "### Technical Quality\n",
        "\n",
        "\n",
        "The technical quality of the paper is good for several reasons.\n",
        "\n",
        "1.   The paper used standard practices for showcasing the test results from the various algorithms used.\n",
        "For the purposes clean testing, the NIST databases were combined and modified to have texts from first 250 writers as training and the other 250 writers as testing. These were stored in a new database called MNIST. The paper trained all the algorithms on the same training data from the and showed the comparisons of every algorithm on the test sets based on: Error rate, memory usage and rejection performance and number of multiply-accumulate operations.\n",
        "\n",
        "<img src=\"https://github.com/katyalrajat/UTS_ML2019_ID13132903/blob/master/test.png?raw=1\">[Figure 3: Error rate on test set for various classification methods]\n",
        "\n",
        "2.  The different techniques used for model learning and optimisation in CNN , SDNN and GTN were well described. For example, in case of a CNN the **Loss function, squashing function and the Euclidian Radical Bias function** have been clearly called out with their workings explained.\n",
        "While in case of a GTN, the training architecture for character string recognition has been shown for weighing in heuristics over segmentation (see below) which is in line with its theoretical explanation.\n",
        "\n",
        "<img src=\"https://github.com/katyalrajat/UTS_ML2019_ID13132903/blob/master/gtn.png?raw=1\">[Figure 4: GTN Loss function architecture for weighing in heuristics over segmentation]\n",
        "\n",
        "3. The paper workings are well understood and replicable in nature. This was shown by the AlexNet algorithm in 2012 which was somewhat a deeper and wider version of the LeNet and won the popular ImageNet competition, which was in some way the re-discovery of the convolutional neural networks. The AlexNet architecture is shown below for comparison with LeNet-5 shown previously[5].\n",
        "\n",
        "      ![alt text](https://cdn-images-1.medium.com/max/800/0*vsi8JJFV_O6Z34ks.png)[Figure 5: AlexNet Architecture , source: medium.com]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn1LcrYi1PUf",
        "colab_type": "text"
      },
      "source": [
        "### Application and X-factor\n",
        "\n",
        "The application scope of this paper is very evident from the over [20,000 citations](https://scholar.google.com.au/scholar?q=Gradient+Based+Learning+Applied+to+Document+Recognition&hl=en&as_sdt=0&as_vis=1&oi=scholart) that it has received most of which have been in the last decade as the field of Deep Learning has evolved.\n",
        "\n",
        "Today, it is said that over 100 hours of content is uploaded every minute on YouTube alone[4]. With the increasing smartphone availability and camera features, most of the data storage is moving towards video and images. This massive incline in media coupled with the increasing computing capabilities has resulted in the creation of interesting applications for image, speech and video pattern recognition and the field of deep learning.\n",
        "\n",
        "At the core of deep learning for vision problems is this concept of a convolutional neural network designed by the authors. The CNNs have been re-designed and optimised for several tasks and become much faster and efficient today. For network wide applications they can be measured on the Accuracy per parameter score that considers the computation cost as well. Some of the most popular versions of the CNNs are shown below, measure on Accuracy per parameters.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*rokvtiIyLgMqkJiORkaLvA.png)[Figure 6: Different Neural network algorithms Accuracy/parameter scores , source: medium.com]\n",
        "\n",
        "Today it is possible to do several recognition-based tasks based on the research. In cases of images, it's possible to have face recognition, object recognition, remote sensing applications, Expression/Mood detection. \n",
        "\n",
        "In case of videos or CCTV you could use it for real-time Violence/Abuse detection, theft detection, video theme detection, video categorisation.\n",
        "\n",
        "In case of speech you can use it for voice recognition applications, Voice based mood detection, voice language detection. \n",
        "\n",
        "The paper already shows how it can read into documents for character string detection. This could be used for storing hand-written textbooks and documents into digital formats for sharing/preserving. \n",
        "\n",
        "It could be used in biology for detecting odd cell/disease patterns.\n",
        "\n",
        "There could be countless applications that deepened on a pattern recognition engine. I would rate this paper as excellent for Application and X-factors. The hint of the wide-spread application of this research paper in the current data science scenario is why I chose to review the paper in the first place.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "law981ws1Unn",
        "colab_type": "text"
      },
      "source": [
        "### Presentation\n",
        "\n",
        "The overall presentation of the paper was good. The authors have showcased the thorough research through clear explanations. The paper is appropriately divided into 11 main sections and further subsections for each major topic.\n",
        "\n",
        "Personally, I liked how the paper has effectively referenced every concept that has been introduced. This allows the reader to dig much deeper into something that they don't understand or something that they feel particularly interested in learning more about.\n",
        "\n",
        "Perhaps an improvement could be to explain the core concepts for single character recognition system(up to section 3) in one paper; while referencing it to explain character string recognition system in another paper. This would best split the paper into two dependent but different problems while helping the reader to grasp the entire paper without taking breaks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyg2UC3R1ZXd",
        "colab_type": "text"
      },
      "source": [
        "### References\n",
        "\n",
        "\n",
        "1.   Cognitron: A self-organizing multilayered neural network, Biological Cybernetics [0340-1200] Fukushima, K yr:1975 vol:20 iss:3-4 pg:121\n",
        "\n",
        "2.   Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position, Biological Cybernetics [0340-1200] Fukushima, K yr:1975 vol:20 iss:3-4 pg:121\n",
        "\n",
        "3. The perception of multiple objects : a connectionist approach, Cambridge, Mass. : MIT Press, by Mozer, Michael C, yr:1990 pg:189-212\n",
        "\n",
        "4. Youtube.com, Quora.com\n",
        "\n",
        "5.  cs231n.stanford.edu\n"
      ]
    }
  ]
}